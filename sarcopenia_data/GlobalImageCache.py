"""
Global Image Cache for K-Fold Cross-Validation

Manages centralized image caching to avoid redundant disk I/O across multiple folds.
Loads all training images once at startup and shares them across all folds.

Benefits:
- 5x faster for 5-fold CV (load once instead of 5 times)
- Same memory usage as per-fold caching
- Clean integration with existing SarcopeniaCSVDataSet

Author: Optimized for MM-CL ASMI Regression Project
"""

import os
import sys
import numpy as np
import pandas as pd
import torch
from tqdm import tqdm
from typing import Dict, Optional
from commons.constant import PATH


class GlobalImageCache:
    """
    Centralized image cache for K-fold cross-validation.

    This cache loads all training images once at startup and shares them
    across all folds, eliminating redundant disk I/O operations.

    Usage Example:
        # Initialize before fold loop in train.py
        all_train_data = pd.read_csv('data/train.csv')
        global_cache = GlobalImageCache(
            csv_data=all_train_data,
            input_size=(224, 224),
            load_images=True
        )

        # Pass to dataset constructors in each fold
        for fold in range(5):
            train_dataset = SarcopeniaCSVDataSet(
                fold_train_data,
                shared_cache=global_cache
            )
            val_dataset = SarcopeniaCSVDataSet(
                fold_val_data,
                shared_cache=global_cache
            )

    Memory Usage:
        - 1200 images Ã— 224Ã—224Ã—3 Ã— 4 bytes â‰ˆ 540 MB
        - Same as per-fold caching, but loaded only once

    Performance:
        - Per-fold caching: 5 folds Ã— 60s = 300s total loading time
        - Global caching: 1 Ã— 60s = 60s total loading time
        - Speedup: 5x for 5-fold cross-validation
    """

    def __init__(self, csv_data: pd.DataFrame, input_size: tuple = (224, 224),
                 load_images: bool = True):
        """
        Initialize the global image cache.

        Args:
            csv_data: Complete training CSV data (all folds combined)
            input_size: Target image size (height, width)
            load_images: Whether to immediately load images into cache
        """
        self._cache: Dict[str, torch.Tensor] = {}
        self._input_size = input_size

        if load_images:
            self._preload_all_images(csv_data)

    def _preload_all_images(self, csv_data: pd.DataFrame):
        """
        Load all unique images from CSV data into memory.

        This method:
        1. Extracts unique image paths from CSV
        2. Loads each DICOM image using SarcopeniaCSVDataSet logic
        3. Converts to tensor and stores in cache
        4. Reports progress and memory usage

        Args:
            csv_data: pandas DataFrame containing IMG_PATH column
        """
        from sarcopenia_data.SarcopeniaDataLoader import SarcopeniaCSVDataSet

        print(f"\n Loading global image cache...")

        # Extract unique image paths (skip NaN and empty strings)
        if PATH not in csv_data.columns:
            print(f"Warning: '{PATH}' column not found in CSV data")
            return

        valid_paths = csv_data[PATH].dropna()
        valid_paths = valid_paths[valid_paths.str.strip() != '']
        unique_paths = valid_paths.unique()

        print(f"   Found {len(unique_paths)} unique images to cache")

        # Create temporary dataset instance to use its load_dicom_image method
        temp_dataset = SarcopeniaCSVDataSet(
            csv_data=csv_data.head(1),  # Dummy data for initialization
            input_size=self._input_size,
            augment=False,
            text_only=False
        )

        # Load each image with progress bar
        failed_count = 0
        for img_path in tqdm(unique_paths, desc="Caching images", unit="img"):
            try:
                # Adjust path for relative execution (same logic as SarcopeniaCSVDataSet)
                adjusted_path = img_path
                if not os.path.isabs(img_path) and not os.path.exists(img_path):
                    # Try with ../../ prefix for execution from driver/reg_driver/
                    potential_path = os.path.join("../../", img_path)
                    if os.path.exists(potential_path):
                        adjusted_path = potential_path

                # Load DICOM and convert to tensor
                img_tensor = temp_dataset.load_dicom_image(adjusted_path)

                # Cache with ORIGINAL path as key (for consistency with dataset lookup)
                self._cache[img_path] = img_tensor
            except Exception as e:
                failed_count += 1
                if failed_count <= 5:  # Only show first 5 errors
                    print(f"\  Failed to load {img_path}: {e}")
                    if failed_count == 5:
                        print(f"   (suppressing further error messages...)")

        # Report results
        print(f"\n Global cache ready:")
        print(f"   - Cached images: {len(self._cache)}")
        print(f"   - Failed loads: {failed_count}")
        print(f"   - Memory usage: {self.estimate_memory_mb():.1f} MB")

        if failed_count > 0:
            print(f"     {failed_count} images will be loaded from disk during training")

    def get(self, img_path: str) -> Optional[torch.Tensor]:
        """
        Retrieve a cached image tensor.

        Args:
            img_path: Path to the image file

        Returns:
            torch.Tensor if image is cached, None otherwise
        """
        return self._cache.get(img_path)

    def size(self) -> int:
        """
        Get the number of cached images.

        Returns:
            Number of images in cache
        """
        return len(self._cache)

    def estimate_memory_mb(self) -> float:
        """
        Estimate the memory usage of the cache in megabytes.

        Returns:
            Estimated memory usage in MB
        """
        if not self._cache:
            return 0.0

        # Calculate memory for one image
        sample_tensor = next(iter(self._cache.values()))
        bytes_per_image = sample_tensor.element_size() * sample_tensor.nelement()

        # Total memory = bytes per image Ã— number of images
        total_bytes = bytes_per_image * len(self._cache)

        return total_bytes / (1024 * 1024)

    def clear(self):
        """
        Clear all cached images to free memory.

        This is typically not needed as the cache will be garbage collected
        when the training script ends.
        """
        self._cache.clear()
        print(f"ðŸ§¹ Global image cache cleared")

    def get_summary(self) -> Dict[str, any]:
        """
        Get summary statistics for the cache.

        Returns:
            Dictionary with cache statistics
        """
        return {
            'cached_images': len(self._cache),
            'memory_mb': self.estimate_memory_mb(),
            'memory_gb': self.estimate_memory_mb() / 1024,
            'input_size': self._input_size
        }
